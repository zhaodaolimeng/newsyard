{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "regex_url = re.compile(\n",
    "        r'^https?://'  # http:// or https://\n",
    "        r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\\.)+[A-Z]{2,6}\\.?|'  # domain...\n",
    "        r'localhost|'  # localhost...\n",
    "        r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3})' # ...or ip\n",
    "        r'(?::\\d+)?'  # optional port\n",
    "        r'(?:/?|[/?]\\S+)$', re.IGNORECASE)\n",
    "\n",
    "def strQ2B(ustring):\n",
    "    rstring = \"\"\n",
    "    for uchar in ustring:\n",
    "        inside_code=ord(uchar)\n",
    "        if inside_code == 12288:                              #全角空格直接转换            \n",
    "            inside_code = 32 \n",
    "        elif (inside_code >= 65281 and inside_code <= 65374): #全角字符（除空格）根据关系转化\n",
    "            inside_code -= 65248\n",
    "        rstring += chr(inside_code)\n",
    "    return rstring\n",
    "\n",
    "# 使用正则和简单匹配检查句子\n",
    "def sentence_ok(sentence):\n",
    "    if not isinstance(sentence, str):\n",
    "        return False\n",
    "    if regex_url.search(sentence) is not None:\n",
    "        return False\n",
    "    if '【系统温馨提示】' in sentence or \\\n",
    "        '5星' in sentence or \\\n",
    "        '亲，欢迎来到联想服务' in sentence or\\\n",
    "        '我是联想在线工程师' in sentence:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# 载入停词\n",
    "def load_stopword_set():\n",
    "    stop_words = []\n",
    "    with codecs.open(\"dataset/stopwords.txt\", \"r\", \"utf-8\") as stop_file:\n",
    "        stop_words = set([w[:-1] for w in stop_file.readlines()])\n",
    "    return stop_words\n",
    "\n",
    "# 全角转换成半角\n",
    "def strQ2B(ustring):\n",
    "    rstring = \"\"\n",
    "    for uchar in ustring:\n",
    "        inside_code=ord(uchar)\n",
    "        if inside_code == 12288:                              #全角空格直接转换            \n",
    "            inside_code = 32 \n",
    "        elif (inside_code >= 65281 and inside_code <= 65374): #全角字符（除空格）根据关系转化\n",
    "            inside_code -= 65248\n",
    "        rstring += chr(inside_code)\n",
    "    return rstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 1\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# 将END标记之前的条目整合\n",
    "import codecs\n",
    "import mysql.connector as c\n",
    "\n",
    "def load_corpus():\n",
    "    texts = []\n",
    "    ids = []\n",
    "    labels = [] # 标签\n",
    "    term_freq = {} # {index: count}\n",
    "    \n",
    "    conn = c.connect(user='root', password='YsuKeg@20160705', host='139.129.208.70', database='qa')\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT `qaid`, `role`, `label`, `end`, `sendtime`, `words` FROM qas\");\n",
    "    stopwords = set(load_stopword_set()) # 停止词\n",
    "\n",
    "    session = []\n",
    "    last_case_id = -1\n",
    "    after_end = False\n",
    "    result_cnt = 0;\n",
    "    \n",
    "    result = cursor.fetchmany(size=10000)\n",
    "    while result:\n",
    "        result_cnt += 1\n",
    "        print('Processing: ' + str(result_cnt))\n",
    "        for case_id, role, label, end_mark, sendtime, raw_sentence in result:\n",
    "            if case_id != last_case_id:\n",
    "                after_end = False\n",
    "            if after_end:\n",
    "                continue\n",
    "            if sentence_ok(raw_sentence):                \n",
    "                session.append(raw_sentence)\n",
    "            if end_mark == 1:\n",
    "                after_end = True\n",
    "                ids.append(last_case_id)\n",
    "                labels.append(label)\n",
    "                texts.append(' '.join(session).rstrip())\n",
    "                session = []\n",
    "            last_case_id = case_id\n",
    "        result = cursor.fetchmany(size=10000)\n",
    "\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    return texts, ids, labels, term_freq\n",
    "\n",
    "raw_texts, raw_ids, raw_labels, term_freq = load_corpus()\n",
    "print('Done!')\n",
    "\n",
    "with codecs.open('tmp/raw_text.txt', 'w') as f:\n",
    "    for text in raw_texts:\n",
    "        f.write(text.strip().replace('\\n', ' ').replace('\\r', ' ') + '\\n')\n",
    "\n",
    "with codecs.open('tmp/raw_ids.txt', 'w') as f:\n",
    "    for idx in raw_ids:\n",
    "        f.write(str(idx) + '\\n')\n",
    "\n",
    "with codecs.open('tmp/raw_labels.txt', 'w') as f:\n",
    "    for label in raw_labels:\n",
    "        f.write(label + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "import codecs\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from scipy.sparse import hstack, vstack\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#################\n",
    "# 特征提取方法\n",
    "#################\n",
    "class FeatureBuilder:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.tf_vectorizer = CountVectorizer(min_df=1)\n",
    "        self.bigram_vectorizer = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "        self.tfidf_vectorizer = TfidfVectorizer(min_df=1)\n",
    "#         self.lda = LatentDirichletAllocation(\n",
    "#             n_topics=16, max_iter=10, learning_method='online', learning_offset=50., random_state=0)\n",
    "        self.ch2 = SelectKBest(chi2, k=6000)\n",
    "\n",
    "    \"\"\"\n",
    "    bi-gram, tf-idf属性既能达到最高精度\n",
    "    其他属性加入之后精度变化不显著\n",
    "    \"\"\"\n",
    "    def fit_transform(self, corpus, labels):\n",
    "        X_1 = self.tf_vectorizer.fit_transform(corpus) # 词频属性\n",
    "        X_2 = self.bigram_vectorizer.fit_transform(corpus) # bi-gram属性\n",
    "        X_3 = self.tfidf_vectorizer.fit_transform(corpus) # tf-idf属性\n",
    "#         X_4 = self.lda.fit_transform(X_1) # 主题属性\n",
    "#         X_hub = hstack([X_1, X_2, X_3, X_4])\n",
    "        X_hub = hstack([X_1, X_2, X_3])\n",
    "        return self.ch2.fit_transform(X_hub, labels)\n",
    "    \n",
    "    \"\"\"\n",
    "    将transform分离，防止数据污染\n",
    "    \"\"\" \n",
    "    def transform(self, corpus):\n",
    "        X_1 = self.tf_vectorizer.transform(corpus) # 词频属性\n",
    "        X_2 = self.bigram_vectorizer.transform(corpus) # bi-gram属性\n",
    "        X_3 = self.tfidf_vectorizer.transform(corpus) # tf-idf属性\n",
    "#         X_4 = self.lda.transform(X_1) # 主题属性\n",
    "#         X_hub = hstack([X_1, X_2, X_3, X_4])\n",
    "        X_hub = hstack([X_1, X_2, X_3])        \n",
    "        return self.ch2.transform(X_hub)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # create fake file\n",
    "\n",
    "# raw_ids = []\n",
    "# with codecs.open('tmp/raw_ids.txt', 'r') as f:\n",
    "#     for raw_id in f.read().splitlines():\n",
    "#         raw_ids.append(raw_id)\n",
    "# with codecs.open('tmp/rule_to_feature.txt', 'w') as f:\n",
    "#     for idx in raw_ids:\n",
    "#         f.write(idx + ' 1 0\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_dict = {}\n",
    "with codecs.open('tmp/rule_to_feature.txt', 'r') as f:\n",
    "    for line in f.read().splitlines():\n",
    "        data = line[:-1].split(' ')\n",
    "        feature_dict[data[0]] = data[1:]\n",
    "\n",
    "corpus, labels, idx_list = [], [], []\n",
    "rule_feature = []\n",
    "\n",
    "with codecs.open('tmp/raw_text.txt', 'r') as f:\n",
    "    for txt in f.readlines():\n",
    "        corpus.append(' '.join(jieba.cut(strQ2B(str(txt[:-1])))))\n",
    "\n",
    "with codecs.open('tmp/raw_labels.txt', 'r') as f:\n",
    "    for label in f.read().splitlines():\n",
    "        labels.append(label)\n",
    "\n",
    "with codecs.open('tmp/raw_ids.txt', 'r') as f:\n",
    "    for raw_id in f.read().splitlines():\n",
    "        idx_list.append(raw_id)\n",
    "        rule_feature.append([int(item) for item in feature_dict[raw_id]])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building features ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/model_selection/_split.py:581: Warning: The least populated class in y has only 4 members, which is too few. The minimum number of groups for any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "k should be >=0, <= n_features; got 6000.Use k='all' to return all features.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-3312cda396f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Building features ...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mfb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFeatureBuilder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mrule_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrule_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mrule_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrule_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-1e092c5c225e>\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, corpus, labels)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m#         X_hub = hstack([X_1, X_2, X_3, X_4])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mX_hub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mch2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_hub\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \"\"\"\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    327\u001b[0m                             % (self.score_func, type(self.score_func)))\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0mscore_func_ret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_func_ret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py\u001b[0m in \u001b[0;36m_check_params\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    472\u001b[0m             raise ValueError(\"k should be >=0, <= n_features; got %r.\"\n\u001b[1;32m    473\u001b[0m                              \u001b[0;34m\"Use k='all' to return all features.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m                              % self.k)\n\u001b[0m\u001b[1;32m    475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_support_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: k should be >=0, <= n_features; got 6000.Use k='all' to return all features."
     ]
    }
   ],
   "source": [
    "#################\n",
    "# 交叉验证性能\n",
    "#################\n",
    "X_zipped = list(zip(corpus, rule_feature, idx_list))\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "\n",
    "overall_train_acc = []\n",
    "overall_test_acc = []\n",
    "\n",
    "for train, test in skf.split(X_zipped, labels):\n",
    "    \n",
    "    corpus_train, rule_train, idx_train = zip(*[X_zipped[i] for i in train])\n",
    "    y_train = [labels[i] for i in train]\n",
    "    corpus_test, rule_test, idx_test = zip(*[X_zipped[i] for i in test])\n",
    "    y_test = [labels[i] for i in test]\n",
    "    \n",
    "    print('Building features ...')\n",
    "    fb = FeatureBuilder()\n",
    "    X_train = fb.fit_transform(corpus_train, y_train)\n",
    "    rule_train = np.array(rule_train)\n",
    "    rule_test = np.array(rule_test)\n",
    "    \n",
    "    X_train_mixed = hstack([X_train, rule_train])\n",
    "    X_test = fb.transform(corpus_test)\n",
    "    X_test_mixed = hstack([X_test, rule_test])\n",
    "    print('Features done!')\n",
    "    \n",
    "    clf = linear_model.LogisticRegression()\n",
    "    clf.fit(X_train_mixed, y_train)\n",
    "    \n",
    "    train_acc = clf.score(X_train_mixed, y_train)\n",
    "    overall_train_acc.append(train_acc)\n",
    "    print('Train accuracy:  %0.3f' % train_acc)\n",
    "\n",
    "    pred = clf.predict(X_test_mixed)\n",
    "    test_acc = metrics.accuracy_score(y_test, pred)\n",
    "    overall_test_acc.append(test_acc)\n",
    "    print(\"Test accuracy:  %0.3f\" % test_acc)\n",
    "\n",
    "print('Overall train acc = %0.3f' % np.mean(overall_train_acc))\n",
    "print('Overall test acc = %0.3f' % np.mean(overall_test_acc))\n",
    "\n",
    "with codecs.open('tmp/result.txt', 'w') as f:\n",
    "    f.write(str(np.mean(overall_test_acc) * 100))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
